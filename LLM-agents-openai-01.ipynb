{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain langchain-openai langchainhub langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading model and embedder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "embedder = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "\n",
    "# load document from arxiv\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2312.10997\")\n",
    "paper = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U faiss-cpu\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "# text splits for the document\n",
    "chunks_paper = text_splitter.split_documents(paper)\n",
    "\n",
    "# storing in vector store\n",
    "faiss_db_paper = FAISS.from_documents(chunks_paper, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating retriever\n",
    "\n",
    "retriever_paper = faiss_db_paper.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting the retriever to a Tool\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever_paper, \"RAG doc\", \"Search for info on RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract—Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs’ intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art\\n\\nIndex Terms—Large language model, retrieval-augmented gen-\\n\\neration, natural language processing, information retrieval\\n\\nI. INTRODUCTION\\n\\nL ARGE language models (LLMs) have achieved remark-\\n\\nable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions.\\n\\nV. AUGMENTATION PROCESS IN RAG\\n\\nIn the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insuffi- cient for complex problems demanding multi-step reasoning, as it provides a limited scope of information [105]. Many studies have optimized the retrieval process in response to this issue, and we have summarised them in Figure 5.\\n\\nA. Iterative Retrieval\\n\\nIterative retrieval is a process where the knowledge base is repeatedly searched based on the initial query and the text generated so far, providing a more comprehensive knowledge\\n\\n10\\n\\nbase for LLMs. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may be affected by semantic discon- tinuity and the accumulation of irrelevant information. ITER- lever- RETGEN [14] employs a synergistic approach that ages “retrieval-enhanced generation” alongside “generation- enhanced retrieval” for tasks that necessitate the reproduction of specific information. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of improved responses in subsequent iterations.\\n\\nB. Recursive Retrieval'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it!\n",
    "\n",
    "retriever_tool.invoke(\"what is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load multiple tools\n",
    "\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# arxiv tool\n",
    "arxiv =  ArxivAPIWrapper()\n",
    "\n",
    "# wikipedia tool\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## api key for tools\n",
    "\n",
    "# serpapi\n",
    "os.environ['SERPAPI_API_KEY']=getpass.getpass()\n",
    "# tavily\n",
    "os.environ['TAVILY_API_KEY']=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langchain api key\n",
    "LANGCHAIN_TRACING_V2=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: tavily-python in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (0.2.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (0.1.63)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->tavily-python) (2024.5.15)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-community tavily-python\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "# tavily tool\n",
    "tavily = TavilySearchResults(max_results=1)\n",
    "\n",
    "# serpapi tool\n",
    "serpapi = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "## A proper name and a good description helps to know on how to use the tools, otherwise ToolExecutor will not work\n",
    "wiki_tool = Tool.from_function(\n",
    "    func=wikipedia.run,\n",
    "    name=\"wiki\",\n",
    "    description=\"useful for when you need to search certain topic on Wikipedia, aka wiki\")\n",
    "\n",
    "arxiv_tool = Tool.from_function(\n",
    "    func=arxiv.run,\n",
    "    name=\"arxiv\",\n",
    "    description=\"useful for querying from arxiv repository\")\n",
    "\n",
    "tavily_tool = Tool.from_function(\n",
    "    func= tavily.invoke,\n",
    "    name = 'tavily',\n",
    "    description = 'Search Engine'\n",
    ")\n",
    "\n",
    "serpapi_tool = Tool.from_function(\n",
    "    func= serpapi.run,\n",
    "    name = 'serpapi',\n",
    "    description = 'Search Engine'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# List of all tools\n",
    "tools = [wiki_tool, arxiv_tool, tavily_tool, serpapi_tool, retriever_tool] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q httpx\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation # reinstall langgraph if fails!\n",
    "\n",
    "# This a helper class we have that is useful for running tools\n",
    "# It takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract—Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs’ intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art\\n\\nIndex Terms—Large language model, retrieval-augmented gen-\\n\\neration, natural language processing, information retrieval\\n\\nI. INTRODUCTION\\n\\nL ARGE language models (LLMs) have achieved remark-\\n\\nable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions.\\n\\nbase for LLMs. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may be affected by semantic discon- tinuity and the accumulation of irrelevant information. ITER- lever- RETGEN [14] employs a synergistic approach that ages “retrieval-enhanced generation” alongside “generation- enhanced retrieval” for tasks that necessitate the reproduction of specific information. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of improved responses in subsequent iterations.\\n\\nB. Recursive Retrieval\\n\\nV. AUGMENTATION PROCESS IN RAG\\n\\nIn the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insuffi- cient for complex problems demanding multi-step reasoning, as it provides a limited scope of information [105]. Many studies have optimized the retrieval process in response to this issue, and we have summarised them in Figure 5.\\n\\nA. Iterative Retrieval\\n\\nIterative retrieval is a process where the knowledge base is repeatedly searched based on the initial query and the text generated so far, providing a more comprehensive knowledge\\n\\n10'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "invocation = ToolInvocation(tool=\"RAG doc\", tool_input=\"What is Retrieval Augmented Generation?\")\n",
    "\n",
    "tool_executor.invoke(invocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt ---> \n",
      " input_variables=['agent_scratchpad', 'chat_history', 'input'] partial_variables={'tools': ['wiki', 'arxiv', 'tavily', 'serpapi', 'RAG doc'], 'tool_names': 'wiki, arxiv, tavily, serpapi, RAG doc'} template=\"\\n### [INST]\\n\\nAssistant is a large language model to answer questions on Retrieval Augmented Generation.\\n\\n\\nContext:\\n------\\n\\nAssistant has access to the following tools:\\n\\n{tools}\\n\\nTo use a tool, please use the following format:\\n\\n'''\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n'''\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n'''\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n'''\\n\\nBegin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n\\nCurrent Scratchpad:\\n{agent_scratchpad}\\n\\n[/INST]\\n \"\n"
     ]
    }
   ],
   "source": [
    "## PROMPT TEMPLATE\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "## set up memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "\n",
    "Assistant is a large language model to answer questions on Retrieval Augmented Generation.\n",
    "\n",
    "\n",
    "Context:\n",
    "------\n",
    "\n",
    "Assistant has access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "'''\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New input: {input}\n",
    "\n",
    "Current Scratchpad:\n",
    "{agent_scratchpad}\n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'chat_history', 'input', 'tool_names', 'tools'],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(\n",
    "    tools=[t.name for t in tools],\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "print(\"prompt ---> \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n"
     ]
    }
   ],
   "source": [
    "## Creating React Agent\n",
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(model, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               verbose=True,\n",
    "                               handle_parsing_errors=True,\n",
    "                               max_iterations=3,\n",
    "                               return_intermediate_steps=True,\n",
    "                               early_stopping_method=\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m'''\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: wiki\n",
      "Action Input: \"Adaptive RAG\"\u001b[0m\u001b[36;1m\u001b[1;3mPage: Recombination-activating gene\n",
      "Summary: The recombination-activating genes (RAGs) encode parts of a protein complex that plays important roles in the rearrangement and recombination of the genes encoding immunoglobulin and T cell receptor molecules. There are two recombination-activating genes RAG1 and RAG2, whose cellular expression is restricted to lymphocytes during their developmental stages. The enzymes encoded by these genes, RAG-1 and RAG-2, are essential to the generation of mature B cells and T cells, two types of lymphocyte that are crucial components of the adaptive immune system.\n",
      "\n",
      "Page: Ragtime\n",
      "Summary: Ragtime, also spelled rag-time or rag time, is a musical style that had its peak from the 1890s to 1910s. Its cardinal trait is its syncopated or \"ragged\" rhythm. Ragtime was popularized during the early 20th century by composers such as Scott Joplin, James Scott and Joseph Lamb. Ragtime pieces (often called \"rags\") are typically composed for and performed on piano, though the genre has been adapted for a variety of instruments and styles.\n",
      "Ragtime music originated within African-American communities in the late 19th century and became a distinctly American form of popular music. It is closely related to marches. Ragtime pieces usually contain several distinct themes, often arranged in patterns of repeats and reprises. Scott Joplin, known as the \"King of Ragtime\", gained fame through compositions like \"Maple Leaf Rag\" and \"The Entertainer\". Ragtime influenced early jazz, Harlem stride piano, Piedmont blues, and European classical composers such as Erik Satie, Claude Debussy, and Igor Stravinsky. Despite being overshadowed by jazz in the 1920s, ragtime has experienced several revivals, notably in the 1950s and 1970s (the latter renaissance due in large part to the use of \"The Entertainer\" in the film The Sting). The music was distributed primarily through sheet music and piano rolls, with some compositions adapted for other instruments and ensembles.\n",
      "\n",
      "Page: The Entertainer (rag)\n",
      "Summary: \"The Entertainer\" is a 1902 classic piano rag written by Scott Joplin. \n",
      "It was sold first as sheet music by John Stark & Son of St. Louis, Missouri, and in the 1910s as piano rolls that would play on player pianos. The first recording was by blues and ragtime musicians the Blue Boys in 1928, played on mandolin and guitar.\n",
      "As one of the classics of ragtime, it returned to international prominence as part of the ragtime revival in the 1970s, when it was used as the theme music for the 1973 Oscar-winning film The Sting. Composer and pianist Marvin Hamlisch's adaptation reached No. 3 on the Billboard pop chart and spent a week at No. 1 on the easy listening chart in 1974. The Sting was set in the 1930s, a full generation after the end of ragtime's mainstream popularity, thus giving the inaccurate impression that ragtime music was popular at that time.\n",
      "The Recording Industry Association of America ranked it at No. 10 on its \"Songs of the Century\" list.\u001b[0m\u001b[32;1m\u001b[1;3mThought: The search results from Wikipedia did not provide relevant information about \"Adaptive RAG\" in the context of Retrieval Augmented Generation. I should try another tool to find more specific information.\n",
      "\n",
      "Action: serpapi\n",
      "Action Input: \"Adaptive RAG Retrieval Augmented Generation\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage.', 'The system routes the question to the RAG (Retrieval-Augmented Generation) system and retrieves relevant documents to generate an accurate ...', 'In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs ...', 'To evaluate how adaptive RAG performs, we collect questions that the knowledge necessary to answer the questions is absent from LLMs.', \"Today's paper proposes Adaptive-RAG, a new adaptive question answering framework that dynamically selects the most suitable strategy for ...\", 'Developing a Retrieval Augmented Generation (RAG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain Framework. Authors.', 'This approach allows for a flexible strategy, seamlessly transitioning between iterative and single-step retrieval-augmented LLMs, as well as ...', \"This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language ...\", 'RAG leverages Sentence Transformers for their ability to understand and compare the semantic content of sentences. This integration is particularly useful in ...']\u001b[0m\u001b[32;1m\u001b[1;3mAdaptive RAG (Retrieval-Augmented Generation) is an advanced framework that enhances the traditional RAG approach by dynamically assessing the necessity of retrieval. This means it can balance the use of external and internal knowledge more effectively. The system can route questions to the RAG system and retrieve relevant documents to generate accurate responses. It can also dynamically select the most suitable strategy for answering questions, whether that involves iterative or single-step retrieval-augmented generation. This adaptability is particularly useful in situations where the necessary knowledge is not present in the language model's internal parameters, allowing for more accurate and contextually relevant answers.\n",
      "\n",
      "Final Answer: Adaptive RAG is an advanced framework in Retrieval-Augmented Generation that dynamically assesses the necessity of retrieval, balancing the use of external and internal knowledge to generate accurate responses. It can select the most suitable strategy for answering questions, making it adaptable to situations where the required knowledge is not present in the language model's internal parameters.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is Adaptive RAG?',\n",
       " 'chat_history': [],\n",
       " 'output': \"Adaptive RAG is an advanced framework in Retrieval-Augmented Generation that dynamically assesses the necessity of retrieval, balancing the use of external and internal knowledge to generate accurate responses. It can select the most suitable strategy for answering questions, making it adaptable to situations where the required knowledge is not present in the language model's internal parameters.\",\n",
       " 'intermediate_steps': [(AgentAction(tool='wiki', tool_input='Adaptive RAG', log='\\'\\'\\'\\nThought: Do I need to use a tool? Yes\\nAction: wiki\\nAction Input: \"Adaptive RAG\"'),\n",
       "   'Page: Recombination-activating gene\\nSummary: The recombination-activating genes (RAGs) encode parts of a protein complex that plays important roles in the rearrangement and recombination of the genes encoding immunoglobulin and T cell receptor molecules. There are two recombination-activating genes RAG1 and RAG2, whose cellular expression is restricted to lymphocytes during their developmental stages. The enzymes encoded by these genes, RAG-1 and RAG-2, are essential to the generation of mature B cells and T cells, two types of lymphocyte that are crucial components of the adaptive immune system.\\n\\nPage: Ragtime\\nSummary: Ragtime, also spelled rag-time or rag time, is a musical style that had its peak from the 1890s to 1910s. Its cardinal trait is its syncopated or \"ragged\" rhythm. Ragtime was popularized during the early 20th century by composers such as Scott Joplin, James Scott and Joseph Lamb. Ragtime pieces (often called \"rags\") are typically composed for and performed on piano, though the genre has been adapted for a variety of instruments and styles.\\nRagtime music originated within African-American communities in the late 19th century and became a distinctly American form of popular music. It is closely related to marches. Ragtime pieces usually contain several distinct themes, often arranged in patterns of repeats and reprises. Scott Joplin, known as the \"King of Ragtime\", gained fame through compositions like \"Maple Leaf Rag\" and \"The Entertainer\". Ragtime influenced early jazz, Harlem stride piano, Piedmont blues, and European classical composers such as Erik Satie, Claude Debussy, and Igor Stravinsky. Despite being overshadowed by jazz in the 1920s, ragtime has experienced several revivals, notably in the 1950s and 1970s (the latter renaissance due in large part to the use of \"The Entertainer\" in the film The Sting). The music was distributed primarily through sheet music and piano rolls, with some compositions adapted for other instruments and ensembles.\\n\\nPage: The Entertainer (rag)\\nSummary: \"The Entertainer\" is a 1902 classic piano rag written by Scott Joplin. \\nIt was sold first as sheet music by John Stark & Son of St. Louis, Missouri, and in the 1910s as piano rolls that would play on player pianos. The first recording was by blues and ragtime musicians the Blue Boys in 1928, played on mandolin and guitar.\\nAs one of the classics of ragtime, it returned to international prominence as part of the ragtime revival in the 1970s, when it was used as the theme music for the 1973 Oscar-winning film The Sting. Composer and pianist Marvin Hamlisch\\'s adaptation reached No. 3 on the Billboard pop chart and spent a week at No. 1 on the easy listening chart in 1974. The Sting was set in the 1930s, a full generation after the end of ragtime\\'s mainstream popularity, thus giving the inaccurate impression that ragtime music was popular at that time.\\nThe Recording Industry Association of America ranked it at No. 10 on its \"Songs of the Century\" list.'),\n",
       "  (AgentAction(tool='serpapi', tool_input='Adaptive RAG Retrieval Augmented Generation\"\\n', log='Thought: The search results from Wikipedia did not provide relevant information about \"Adaptive RAG\" in the context of Retrieval Augmented Generation. I should try another tool to find more specific information.\\n\\nAction: serpapi\\nAction Input: \"Adaptive RAG Retrieval Augmented Generation\"\\n'),\n",
       "   '[\\'Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage.\\', \\'The system routes the question to the RAG (Retrieval-Augmented Generation) system and retrieves relevant documents to generate an accurate ...\\', \\'In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs ...\\', \\'To evaluate how adaptive RAG performs, we collect questions that the knowledge necessary to answer the questions is absent from LLMs.\\', \"Today\\'s paper proposes Adaptive-RAG, a new adaptive question answering framework that dynamically selects the most suitable strategy for ...\", \\'Developing a Retrieval Augmented Generation (RAG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain Framework. Authors.\\', \\'This approach allows for a flexible strategy, seamlessly transitioning between iterative and single-step retrieval-augmented LLMs, as well as ...\\', \"This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs\\'s parametric knowledge is static. RAG allows language ...\", \\'RAG leverages Sentence Transformers for their ability to understand and compare the semantic content of sentences. This integration is particularly useful in ...\\']')]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it !\n",
    "agent_outcome = agent_executor.invoke({\"input\": \"what is Adaptive RAG?\",\"chat_history\": []})\n",
    "agent_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool='serpapi', tool_input='current weather in Montreal\"\\n\\'\\'\\'', log='\\'\\'\\'\\nThought: Do I need to use a tool? Yes\\nAction: serpapi\\nAction Input: \"current weather in Montreal\"\\n\\'\\'\\'')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_action = agent_outcome['intermediate_steps'][0][0]\n",
    "output = tool_executor.invoke(agent_action)\n",
    "agent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent state class\n",
    "\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The input string\n",
    "    input: str\n",
    "    # The list of previous messages in the conversation\n",
    "    chat_history: list[BaseMessage]\n",
    "    # The outcome of a given call to the agent\n",
    "    # Needs `None` as a valid type, since this is what this will start as\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    # List of actions and corresponding observations\n",
    "    # Here we annotate this with `operator.add` to indicate that operations to\n",
    "    # this state should be ADDED to the existing values (not overwrite it)\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes\n",
    "from langchain_core.agents import AgentActionMessageLog\n",
    "\n",
    "def run_agent(data):\n",
    "    inputs = data.copy()\n",
    "    text = inputs['input']\n",
    "    # chat_history = inputs['chat_history']\n",
    "    agent_outcome = agent_executor.invoke({\"input\":text}) #, \"chat_history\": chat_history\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def execute_tools(data):\n",
    "    # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
    "    agent_output = data[\"agent_outcome\"]\n",
    "    if len(agent_output['intermediate_steps'])>=1 :\n",
    "        agent_action = agent_output['intermediate_steps'][0][0]\n",
    "        output = tool_executor.invoke(agent_action)\n",
    "        return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
    "    else:\n",
    "        return {\"intermediate_steps\":[]}\n",
    "\n",
    "# Define logic that is used to determine which conditional edge to go down\n",
    "def should_continue(data):\n",
    "    # If the agent outcome is an AgentFinish, then we return `exit` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    if data[\"agent_outcome\"][\"output\"] is not None:\n",
    "        print(\" **AgentFinish** \" )\n",
    "        return \"end\"\n",
    "    # Otherwise, an AgentAction is returned\n",
    "    # Here we return `continue` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    else:\n",
    "        print(\" **continue** \" )\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import CurveStyle, NodeColors, MermaidDrawMethod\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo provide a comprehensive explanation of Adaptive Retrieval methods, I should gather detailed information from reliable sources. I'll start by checking academic papers or articles that discuss Adaptive Retrieval methods.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: Adaptive Retrieval methods\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-03-23\n",
      "Title: Parameter-Efficient Sparse Retrievers and Rerankers using Adapters\n",
      "Authors: Vaishali Pal, Carlos Lassance, Hervé Déjean, Stéphane Clinchant\n",
      "Summary: Parameter-Efficient transfer learning with Adapters have been studied in\n",
      "Natural Language Processing (NLP) as an alternative to full fine-tuning.\n",
      "Adapters are memory-efficient and scale well with downstream tasks by training\n",
      "small bottle-neck layers added between transformer layers while keeping the\n",
      "large pretrained language model (PLMs) frozen. In spite of showing promising\n",
      "results in NLP, these methods are under-explored in Information Retrieval.\n",
      "While previous studies have only experimented with dense retriever or in a\n",
      "cross lingual retrieval scenario, in this paper we aim to complete the picture\n",
      "on the use of adapters in IR. First, we study adapters for SPLADE, a sparse\n",
      "retriever, for which adapters not only retain the efficiency and effectiveness\n",
      "otherwise achieved by finetuning, but are memory-efficient and orders of\n",
      "magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\n",
      "just 2\\% of training parameters, but outperforms fully fine-tuned counterpart\n",
      "and existing parameter-efficient dense IR models on IR benchmark datasets.\n",
      "Secondly, we address domain adaptation of neural retrieval thanks to adapters\n",
      "on cross-domain BEIR datasets and TripClick. Finally, we also consider\n",
      "knowledge sharing between rerankers and first stage rankers. Overall, our study\n",
      "complete the examination of adapters for neural IR\n",
      "\n",
      "Published: 2019-05-29\n",
      "Title: Approaching Adaptation Guided Retrieval in Case-Based Reasoning through Inference in Undirected Graphical Models\n",
      "Authors: Luigi Portinale\n",
      "Summary: In Case-Based Reasoning, when the similarity assumption does not hold, the\n",
      "retrieval of a set of cases structurally similar to the query does not\n",
      "guarantee to get a reusable or revisable solution. Knowledge about the\n",
      "adaptability of solutions has to be exploited, in order to define a method for\n",
      "adaptation-guided retrieval. We propose a novel approach to address this\n",
      "problem, where knowledge about the adaptability of the solutions is captured\n",
      "inside a metric Markov Random Field (MRF). Nodes of the MRF represent cases and\n",
      "edges connect nodes whose solutions are close in the solution space. States of\n",
      "the nodes represent different adaptation levels with respect to the potential\n",
      "query. Metric-based potentials enforce connected nodes to share the same state,\n",
      "since cases having similar solutions should have the same adaptability level\n",
      "with respect to the query. The main goal is to enlarge the set of potentially\n",
      "adaptable cases that are retrieved without significantly sacrificing the\n",
      "precision and accuracy of retrieval. We will report on some experiments\n",
      "concerning a retrieval architecture where a simple kNN retrieval (on the\n",
      "problem description) is followed by a further retrieval step based on MRF\n",
      "inference.\n",
      "\n",
      "Published: 2002-06-24\n",
      "Title: Speech-Driven Text Retrieval: Using Target IR Collections for Statistical Language Model Adaptation in Speech Recognition\n",
      "Authors: Atsushi Fujii, Katunobu Itou, Tetsuya Ishikawa\n",
      "Summary: Speech recognition has of late become a practical technology for real world\n",
      "applications. Aiming at speech-driven text retrieval, which facilitates\n",
      "retrieving information with spoken queries, we propose a method to integrate\n",
      "speech recognition and retrieval methods. Since users speak contents related to\n",
      "a target collection, we adapt statistical language models used for speech\n",
      "recognition based on the target collection, so as to improve both the\n",
      "recognition and retrieval accuracy. Experiments using existing test collections\n",
      "combined with dictated queries showed the effectiveness of our method.\u001b[0m\u001b[32;1m\u001b[1;3mThe three papers retrieved from arXiv provide valuable insights into different aspects of adaptive retrieval methods. However, each paper approaches the concept differently within their specific contexts. I'll synthesize the information from these papers to provide a comprehensive explanation of adaptive retrieval methods.\n",
      "\n",
      "1. **Parameter-Efficient Sparse Retrievers and Rerankers using Adapters**:\n",
      "   - **Summary**: This paper explores the use of adapters in information retrieval (IR), specifically with SPLADE, a sparse retriever. Adapters are small bottle-neck layers added to transformer layers, allowing for memory-efficient and scalable transfer learning. They optimize only a small fraction of parameters (around 2%) while retaining the efficiency and effectiveness of full fine-tuning. The study demonstrates that using adapters can outperform fully fine-tuned models and existing parameter-efficient dense IR models. Additionally, the paper addresses domain adaptation for neural retrieval and knowledge sharing between rerankers and first-stage rankers.\n",
      "   - **Key Points**: Adapters enable efficient training, improve performance, and facilitate domain adaptation in IR.\n",
      "\n",
      "2. **Approaching Adaptation Guided Retrieval in Case-Based Reasoning through Inference in Undirected Graphical Models**:\n",
      "   - **Summary**: This paper discusses adaptation-guided retrieval in Case-Based Reasoning (CBR) when the similarity assumption does not hold. The proposed approach uses a Metric Markov Random Field (MRF) to capture knowledge about the adaptability of solutions. Nodes in the MRF represent cases, and edges connect nodes with similar solutions. The goal is to retrieve a larger set of potentially adaptable cases without sacrificing precision. This is achieved by using a k-Nearest Neighbors (kNN) retrieval followed by MRF inference to refine the results.\n",
      "   - **Key Points**: This method focuses on retrieving cases based on adaptability rather than mere similarity, using a graphical model to enhance retrieval accuracy.\n",
      "\n",
      "3. **Speech-Driven Text Retrieval: Using Target IR Collections for Statistical Language Model Adaptation in Speech Recognition**:\n",
      "   - **Summary**: The paper proposes a method for integrating speech recognition with text retrieval by adapting statistical language models based on target IR collections. This approach aims to improve both recognition and retrieval accuracy by tailoring the language model to the specific content of the target collection. Experiments showed that this method enhances the effectiveness of speech-driven text retrieval.\n",
      "   - **Key Points**: Adapting language models to the target collection improves recognition and retrieval accuracy in speech-driven text retrieval.\n",
      "\n",
      "**Comprehensive Explanation of Adaptive Retrieval Methods**:\n",
      "Adaptive retrieval methods involve modifying or enhancing retrieval systems to improve their performance in specific contexts or tasks. These methods can be broadly categorized into the following approaches:\n",
      "\n",
      "1. **Parameter-Efficient Adaptation**:\n",
      "   - Using adapters, small additional layers that enable efficient transfer learning without the need for extensive retraining of the entire model.\n",
      "   - Example: Adapters for sparse retrievers like SPLADE, which improve efficiency and effectiveness, and facilitate domain adaptation and knowledge sharing.\n",
      "\n",
      "2. **Adaptation-Guided Retrieval**:\n",
      "   - Focusing on the adaptability of retrieved cases rather than just their similarity to the query.\n",
      "   - Example: Using Metric Markov Random Fields to model adaptability in Case-Based Reasoning, enhancing the retrieval of reusable or revisable solutions.\n",
      "\n",
      "3. **Context-Specific Adaptation**:\n",
      "   - Adapting models based on the specific content or context of the target collection to improve performance.\n",
      "   - Example: Adapting statistical language models in speech-driven text retrieval to the target IR collection, improving both speech recognition and retrieval accuracy.\n",
      "\n",
      "Overall, adaptive retrieval methods aim to enhance the relevance and precision of retrieval systems by incorporating adaptability, efficiency, and context-specific adjustments.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3mTo provide a comprehensive explanation of Adaptive Retrieval methods, I'll integrate the key points from the retrieved papers and synthesize a cohesive answer.\n",
      "\n",
      "**Comprehensive Explanation of Adaptive Retrieval Methods**:\n",
      "\n",
      "Adaptive retrieval methods involve modifying or enhancing retrieval systems to improve their performance in specific contexts or tasks. These methods can be broadly categorized into the following approaches:\n",
      "\n",
      "1. **Parameter-Efficient Adaptation**:\n",
      "   - **Using Adapters**: Adapters are small, additional layers added to transformer models that enable efficient transfer learning without the need for extensive retraining of the entire model. This approach is memory-efficient and scales well with downstream tasks.\n",
      "   - **Example**: The use of adapters in SPLADE, a sparse retriever, optimizes only around 2% of the training parameters while retaining the efficiency and effectiveness of full fine-tuning. Adapters facilitate domain adaptation and knowledge sharing between rerankers and first-stage rankers, outperforming fully fine-tuned models and existing parameter-efficient dense IR models.\n",
      "\n",
      "2. **Adaptation-Guided Retrieval**:\n",
      "   - **Focus on Adaptability**: Instead of retrieving cases solely based on their structural similarity to the query, this method considers the adaptability of solutions. It aims to retrieve a larger set of potentially adaptable cases without sacrificing precision.\n",
      "   - **Example**: Using a Metric Markov Random Field (MRF) in Case-Based Reasoning (CBR) to capture knowledge about the adaptability of solutions. Nodes represent cases, and edges connect nodes with similar solutions. This approach, followed by a k-Nearest Neighbors (kNN) retrieval, enhances the retrieval of reusable or revisable solutions.\n",
      "\n",
      "3. **Context-Specific Adaptation**:\n",
      "   - **Adapting to Target Collection**: This approach involves tailoring models based on the specific content or context of the target collection to improve performance. It integrates speech recognition and retrieval methods.\n",
      "   - **Example**: Adapting statistical language models used for speech recognition to the specific content of the target IR collection. This adaptation improves both the recognition and retrieval accuracy in speech-driven text retrieval scenarios.\n",
      "\n",
      "Overall, adaptive retrieval methods aim to enhance the relevance and precision of retrieval systems by incorporating adaptability, efficiency, and context-specific adjustments. These methods address different aspects such as memory efficiency, adaptability of solutions, and context-specific enhancement to improve the performance and accuracy of information retrieval systems.\n",
      "\n",
      "Final Answer: Adaptive retrieval methods involve modifying or enhancing retrieval systems to improve their performance in specific contexts or tasks. These methods include:\n",
      "\n",
      "1. **Parameter-Efficient Adaptation**: Utilizing adapters to add small layers to models, enabling efficient transfer learning and domain adaptation without extensive retraining.\n",
      "2. **Adaptation-Guided Retrieval**: Focusing on retrieving cases based on their adaptability rather than just structural similarity, using graphical models like Metric Markov Random Fields.\n",
      "3. **Context-Specific Adaptation**: Tailoring models to the specific content of the target collection to improve both recognition and retrieval accuracy, especially in speech-driven text retrieval.\n",
      "\n",
      "These methods aim to enhance the relevance, precision, and efficiency of retrieval systems by incorporating adaptability and context-specific adjustments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"Explain Adaptive Retrieval methods\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo explain what Langchain Agents are and to connect them with real-life instances using an appropriate metaphor, it would be beneficial to gather detailed information about Langchain Agents first. \n",
      "\n",
      "Action: wiki\n",
      "Action Input: \"Langchain Agents\"\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework that simplifies the creation of applications using large language models (LLMs). To explain what LangChain Agents are and connect them with real-life instances using an appropriate metaphor, I can proceed with the information at hand.\n",
      "\n",
      "LangChain Agents can be thought of as specialized \"workers\" in a big company (the application) that perform specific tasks using their expertise (language models). Just as in a real-life company where you have different departments or workers specializing in certain tasks (e.g., HR for human resources, IT for tech support, Marketing for promotion, etc.), LangChain Agents are designed to handle specialized tasks such as document analysis, summarization, chatbot functionalities, and code analysis.\n",
      "\n",
      "Here's a metaphorical explanation:\n",
      "\n",
      "Imagine you are the manager of a bustling office (the application) with various tasks that need to be completed efficiently. Instead of trying to do everything yourself, you hire a team of experts (LangChain Agents) who are each highly skilled in particular areas:\n",
      "\n",
      "1. **Document Analyst**: This expert is great at reading through stacks of documents and summarizing key points, just like a LangChain Agent designed for document analysis and summarization.\n",
      "2. **Customer Service Representative**: This worker handles customer inquiries and provides support, akin to a LangChain Agent functioning as a chatbot.\n",
      "3. **Technical Support Specialist**: This employee is adept at diagnosing and fixing software issues, similar to a LangChain Agent that can analyze and debug code.\n",
      "\n",
      "Each agent (worker) in your office uses their specific skills to handle their respective tasks, making the entire operation smoother and more efficient. Similarly, LangChain Agents are specialized components within the framework that leverage large language models to perform specific functionalities, streamlining the development and performance of applications.\n",
      "\n",
      "Final Answer: LangChain Agents are specialized components within the LangChain framework that perform specific tasks using large language models (LLMs). They can be compared to specialized workers in a company, each handling tasks in their area of expertise—like document analysis, customer service, and technical support—thereby making the entire application more efficient and effective.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"Explain what are Langchain Agents. Use appropriate metaphor to connect with real-life instances\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adaptive retrieval methods involve modifying or enhancing retrieval systems to improve their performance in specific contexts or tasks. These methods include:\\n\\n1. **Parameter-Efficient Adaptation**: Utilizing adapters to add small layers to models, enabling efficient transfer learning and domain adaptation without extensive retraining.\\n2. **Adaptation-Guided Retrieval**: Focusing on retrieving cases based on their adaptability rather than just structural similarity, using graphical models like Metric Markov Random Fields.\\n3. **Context-Specific Adaptation**: Tailoring models to the specific content of the target collection to improve both recognition and retrieval accuracy, especially in speech-driven text retrieval.\\n\\nThese methods aim to enhance the relevance, precision, and efficiency of retrieval systems by incorporating adaptability and context-specific adjustments.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['agent_outcome']['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
