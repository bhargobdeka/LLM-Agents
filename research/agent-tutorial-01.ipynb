{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain unstructured[all-docs] langchain-community arxiv wikipedia langgraph langchainhub langchain-nvidia-ai-endpoints==0.0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='ai-arctic', model_type='chat', api_type=None, model_name='snowflake/arctic', client='ChatNVIDIA', path='7408b6b5-09e7-4ae5-a3fe-2db063e4e609'),\n",
       " Model(id='ai-codegemma-7b', model_type='chat', api_type=None, model_name='google/codegemma-7b', client='ChatNVIDIA', path='7dfc10a8-3cc4-448e-97c1-2213308dc222'),\n",
       " Model(id='ai-codellama-70b', model_type='chat', api_type=None, model_name='meta/codellama-70b', client='ChatNVIDIA', path='f6b06895-d073-4714-8bb2-26c09e9f6597'),\n",
       " Model(id='ai-fuyu-8b', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='e598bfc1-b058-41af-869d-556d3c7e1b48'),\n",
       " Model(id='ai-gemma-2b', model_type='chat', api_type=None, model_name='google/gemma-2b', client='ChatNVIDIA', path='04174188-f742-4069-9e72-d77c2b77d3cb'),\n",
       " Model(id='ai-gemma-7b', model_type='chat', api_type=None, model_name='google/gemma-7b', client='ChatNVIDIA', path='a13e3bed-ca42-48f8-b3f1-fbc47b9675f9'),\n",
       " Model(id='ai-google-deplot', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='784a8ca4-ea7d-4c93-bb46-ec027c3fae47'),\n",
       " Model(id='ai-llama2-70b', model_type='chat', api_type=None, model_name='meta/llama2-70b', client='ChatNVIDIA', path='2fddadfb-7e76-4c8a-9b82-f7d3fab94471'),\n",
       " Model(id='ai-llama3-70b', model_type='chat', api_type=None, model_name='meta/llama3-70b-instruct', client='ChatNVIDIA', path='a88f115a-4a47-4381-ad62-ca25dc33dc1b'),\n",
       " Model(id='ai-llama3-8b', model_type='chat', api_type=None, model_name='meta/llama3-8b-instruct', client='ChatNVIDIA', path='a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe'),\n",
       " Model(id='ai-microsoft-kosmos-2', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='6018fed7-f227-48dc-99bc-3fd4264d5037'),\n",
       " Model(id='ai-mistral-7b-instruct-v2', model_type='chat', api_type=None, model_name='mistralai/mistral-7b-instruct-v0.2', client='ChatNVIDIA', path='d7618e99-db93-4465-af4d-330213a7f51f'),\n",
       " Model(id='ai-mistral-large', model_type='chat', api_type=None, model_name='mistralai/mistral-large', client='ChatNVIDIA', path='767b5b9a-3f9d-4c1d-86e8-fa861988cee7'),\n",
       " Model(id='ai-mixtral-8x22b-instruct', model_type='chat', api_type=None, model_name='mistralai/mixtral-8x22b-instruct-v0.1', client='ChatNVIDIA', path='710c92d0-7c98-46d6-b5ae-07e84bcaa5d3'),\n",
       " Model(id='ai-mixtral-8x7b-instruct', model_type='chat', api_type=None, model_name='mistralai/mixtral-8x7b-instruct-v0.1', client='ChatNVIDIA', path='a1e53ece-bff4-44d1-8b13-c009e5bf47f6'),\n",
       " Model(id='ai-neva-22b', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='bc205f8e-1740-40df-8d32-c4321763498a'),\n",
       " Model(id='ai-phi-3-mini', model_type='chat', api_type=None, model_name='microsoft/phi-3-mini-128k-instruct', client='ChatNVIDIA', path='4a58c6cb-a9b4-4014-99de-3e704d4ae687'),\n",
       " Model(id='ai-recurrentgemma-2b', model_type='chat', api_type=None, model_name='google/recurrentgemma-2b', client='ChatNVIDIA', path='2f495340-a99f-4b4b-89bd-1beb003dd896'),\n",
       " Model(id='ai-ai-weather-forecasting', model_type=None, api_type=None, model_name=None, client=None, path='9cec444c-db1c-4525-9c6f-f40e4a5b11ce'),\n",
       " Model(id='ai-arctic-embed-l', model_type=None, api_type=None, model_name=None, client=None, path='1528a0ad-205a-46ac-a783-94e2372586a9'),\n",
       " Model(id='ai-dbrx-instruct', model_type=None, api_type=None, model_name=None, client=None, path='3d6c2ff8-8bfc-4d10-8fd0-b7337288e869'),\n",
       " Model(id='ai-diffdock', model_type=None, api_type=None, model_name=None, client=None, path='f3dda972-561a-4772-8c09-873594b6fb72'),\n",
       " Model(id='ai-esmfold', model_type=None, api_type=None, model_name=None, client=None, path='a68c59e0-47a6-4a50-bf64-6d88766d56bf'),\n",
       " Model(id='ai-google-paligemma', model_type=None, api_type=None, model_name=None, client=None, path='a70e7356-c643-41b3-9a7e-b89ea1e7dea1'),\n",
       " Model(id='ai-molmim-generate', model_type=None, api_type=None, model_name=None, client=None, path='72be0b68-179f-412c-ac03-9a481f78cb9f'),\n",
       " Model(id='ai-nvidia-cuopt', model_type=None, api_type=None, model_name=None, client=None, path='b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0'),\n",
       " Model(id='ai-parakeet-ctc-riva', model_type=None, api_type=None, model_name=None, client=None, path='22164014-a6cc-4a6f-b048-f3a303e745bb'),\n",
       " Model(id='ai-phi-3-medium-4k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='971a7ad5-009e-47a1-941a-22b74d742304'),\n",
       " Model(id='ai-phi-3-mini-4k', model_type=None, api_type=None, model_name=None, client=None, path='ad974453-80d4-46df-a02d-6f7dae20c010'),\n",
       " Model(id='ai-phi-3-small-128k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='2ae1bc0b-a204-42fc-9edc-bd5f7fb2340d'),\n",
       " Model(id='ai-phi-3-small-8k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='5affddf9-9f6a-44f5-baa7-8bc7ab5dc398'),\n",
       " Model(id='ai-phi-3-vision-128k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='20f2537e-8593-4eb9-ad40-60eee3bbaa55'),\n",
       " Model(id='ai-sdxl-turbo', model_type=None, api_type=None, model_name=None, client=None, path='f886140c-424e-4c82-a841-99e23f9ae35d'),\n",
       " Model(id='ai-sea-lion-7b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='02f84bf4-c1a1-489b-a9de-ac3e8dcdec14'),\n",
       " Model(id='ai-seallm-7b', model_type=None, api_type=None, model_name=None, client=None, path='e71e675e-a53d-4f2b-a441-e1287bd17dc8'),\n",
       " Model(id='ai-stable-diffusion-xl', model_type=None, api_type=None, model_name=None, client=None, path='c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2'),\n",
       " Model(id='ai-stable-video-diffusion', model_type=None, api_type=None, model_name=None, client=None, path='8cd594f1-6a4d-4f8f-82b4-d1bf89adae98'),\n",
       " Model(id='ai-vista-3d', model_type=None, api_type=None, model_name=None, client=None, path='72311276-923f-4478-a506-d5b80914728a')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# get all available models\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "llm = ChatNVIDIA(model='ai-llama2-70b') #mistralai/mixtral-8x22b-instruct-v0.1, ai-mistral-large\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# load document from arxiv\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2312.10997\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embedder)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba 11 is designed for personal assistant applications, while Amazon’s Kendra 12 offers intelligent enterprise search', metadata={'source': '/var/folders/8j/ybqbv42143jfglvsf83t5s_c0000gn/T/tmpzmgxrkyv/tmp.10997'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is RAG?\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TAVILY_API_KEY']=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# convert the retriever as a tool\n",
    "retriever_search = create_retriever_tool(retriever, \"RAG doc\", \"Search for info on RAG\")\n",
    "\n",
    "# arxiv tool\n",
    "arxiv =  ArxivAPIWrapper()\n",
    "\n",
    "# tavily tool\n",
    "tavily_search = TavilySearchResults()\n",
    "\n",
    "# wikipedia tool\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba 11 is designed for personal assistant applications, while Amazon’s Kendra 12 offers intelligent enterprise search\\n\\n2) New Patterns: Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace tradi- tional retrieval with LLM-generated content, while Recite- Read [22] emphasizes retrieval from model weights, enhanc- ing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater\\n\\nIndex Terms—Large language model, retrieval-augmented gen-\\n\\neration, natural language processing, information retrieval\\n\\nI. INTRODUCTION\\n\\nL ARGE language models (LLMs) have achieved remark-\\n\\nable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions.\\n\\nAbstract—Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs’ intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriever_search.run(\"what is RAG?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "tavily_tool = Tool.from_function(\n",
    "    func=tavily_search.invoke,\n",
    "    name='tavily',\n",
    "    description='search engine'\n",
    ")\n",
    "retriever_tool = Tool.from_function(\n",
    "    func=retriever_search.invoke,\n",
    "    name='retriever_docs',\n",
    "    description='Tool to retrieve document'\n",
    ")\n",
    "\n",
    "tool_list = [tavily_tool, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-openai) (0.1.52)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain-openai)\n",
      "  Using cached openai-1.30.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (0.1.63)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (8.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.24.0->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.1)\n",
      "Using cached langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Using cached openai-1.30.3-py3-none-any.whl (320 kB)\n",
      "Using cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl (906 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 langchain-openai-0.1.7 openai-1.30.3 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding the tools with LLM\n",
    "model_with_tools = model.bind_tools(tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: Hello! How can I assist you today?\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# now run model with tools\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(model, tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', id='bb164567-9b08-4f74-83b4-f375c410e1a1'),\n",
       " AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 70, 'total_tokens': 80}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8dd0b254-1db2-42e9-861c-05ec7f8559b9-0')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"hi!\")]})\n",
    "\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='explain Retrieval Augmented Generation?', id='1c4fd8d9-f5b7-4c44-a6da-d3a5707e03fa'),\n",
       " AIMessage(content=\"Retrieval Augmented Generation (RAG) is a method that combines the benefits of both the retrieval-based and generation-based models for open-domain question answering tasks. The method leverages the advantages of extractive methods that retrieve relevant documents and generative methods that generate responses in a fine-grained manner. \\n\\nHere's how it works:\\n\\n1. **Document Retrieval**: Given an input query, RAG retrieves a set of documents that are relevant to the query. This is similar to the process used in a search engine, where the system retrieves the most relevant documents or articles related to a search query.\\n\\n2. **Contextual Understanding**: The retrieved documents are then provided as context to a sequence-to-sequence model. This model understands the context from these documents and uses it to generate a response.\\n\\n3. **Response Generation**: The model generates a response based on the understanding it builds from the retrieved documents and the given query.\\n\\nRAG provides a fine-grained combination of retrieval and generation, enabling the model to generate more informed and accurate responses. It allows the model to pull in information from a large corpus of documents, thus expanding its knowledge base and improving its ability to answer open-domain questions.\", response_metadata={'token_usage': {'completion_tokens': 242, 'prompt_tokens': 75, 'total_tokens': 317}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c2a4aa04-486b-4bec-b543-7617fdb20f0a-0')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"explain Retrieval Augmented Generation?\")]}\n",
    ")\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr # what is this?\n",
    "%pip install -U langgraph langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(model, tool_list, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent,\n",
    "                               tools=tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'explain Retrieval Augmented Generation?',\n",
       " 'output': \"Retrieval-Augmented Generation (RAG) is an advanced technique in natural language processing (NLP) that combines retrieval mechanisms with generative models to improve the quality and accuracy of generated text. Here's a breakdown of the concept:\\n\\n### 1. **Retrieval Mechanism**\\n- **Purpose**: The retrieval component fetches relevant documents or pieces of information from a large corpus or database that are related to the input query or context.\\n- **Technology**: Often, this involves using search engines, similarity measures, or trained retrieval models.\\n\\n### 2. **Generative Model**\\n- **Purpose**: The generative model, typically a variant of Transformer models like GPT-3, uses the retrieved information and the input query to generate coherent and contextually accurate text.\\n- **Technology**: These models are trained on large datasets and can produce human-like text based on the input they receive.\\n\\n### 3. **Combination of Both**\\n- **Process**: In RAG, the system first retrieves relevant documents or passages related to the input query. Then, it uses this retrieved information as additional context for the generative model to create more informed and accurate responses.\\n- **Workflow**:\\n  1. **Input Query**: A user provides a query or prompt.\\n  2. **Retrieval**: The system searches and retrieves relevant information from a database.\\n  3. **Augmentation**: The retrieved information is combined with the original query.\\n  4. **Generation**: The generative model processes the augmented input to produce the final output.\\n\\n### 4. **Advantages**\\n- **Enhanced Accuracy**: By grounding the generation in real, retrieved data, the system can provide more accurate and factually correct responses.\\n- **Contextual Relevance**: The generated text is more relevant to the specific query since it uses contextually appropriate information.\\n- **Improved Performance**: This hybrid approach can outperform purely generative or purely retrieval-based systems in many NLP tasks.\\n\\n### 5. **Applications**\\n- **Customer Support**: Generating accurate responses based on a knowledge base.\\n- **Content Creation**: Assisting writers by providing factually correct information.\\n- **Question Answering**: Providing precise answers to user queries by referencing a large corpus of documents.\\n\\nIn essence, RAG leverages the strengths of both retrieval and generation to create a sophisticated system capable of producing high-quality and contextually appropriate text.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"explain Retrieval Augmented Generation?\"\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests->google-search-results) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from requests->google-search-results) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-search-results\n",
    "from langchain_community.utilities import SerpAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor, load_tools, create_tool_calling_agent\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "\n",
    "tools = load_tools([\"serpapi\",\"wikipedia\"],llm=llm)\n",
    "\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(model, tools) # the issue is does not work with open-source model !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor, load_tools\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import (\n",
    "    ReActJsonSingleInputOutputParser,\n",
    ")\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "# setup ReAct style prompt\n",
    "prompt = hub.pull(\"hwchase17/react-json\")\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "# define the agent\n",
    "chat_model_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "# instantiate AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"what is Generative AI?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
