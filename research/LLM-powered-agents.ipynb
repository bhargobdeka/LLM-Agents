{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  set up NVIDIA API KEY and import Chat Models and Embeddings\n",
    "\n",
    "- complete the [setup]{https://python.langchain.com/v0.1/docs/integrations/text_embedding/nvidia_ai_endpoints/#setup} and get the api key.\n",
    "\n",
    "- run the code snippet below and enter your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid NVIDIA_API_KEY already in environment. Delete to reset\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# ## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "# ## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# # del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "# if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "# else:\n",
    "#     nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "#     assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "#     os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to query model endpoint https://api.nvcf.nvidia.com/v2/nvcf/functions.\n[401] Unauthorized\nBearer error=\"invalid_token\"\nerror_description=\"Bearer token is malformed\"\nerror_uri=\"https://tools.ietf.org/html/rfc6750#section-3.1\"\nPlease check or regenerate your API key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:174\u001b[0m, in \u001b[0;36mNVEModel.available_functions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m     invoke_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoints\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url)\n\u001b[0;32m--> 174\u001b[0m     query_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:316\u001b[0m, in \u001b[0;36mNVEModel.query\u001b[0;34m(self, invoke_url, payload, request)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:238\u001b[0m, in \u001b[0;36mNVEModel._get\u001b[0;34m(self, invoke_url, payload)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_inputs)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:303\u001b[0m, in \u001b[0;36mNVEModel._try_raise\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: [401] Unauthorized\nBearer error=\"invalid_token\"\nerror_description=\"Bearer token is malformed\"\nerror_uri=\"https://tools.ietf.org/html/rfc6750#section-3.1\"\nPlease check or regenerate your API key.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_nvidia_ai_endpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NVIDIAEmbeddings\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get all available models\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mChatNVIDIA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_available_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:631\u001b[0m, in \u001b[0;36m_NVIDIAClient.get_available_models\u001b[0;34m(cls, mode, client, list_all, filter, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m nveclient \u001b[38;5;241m=\u001b[39m (client \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\u001b[38;5;241m.\u001b[39mmode(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mclient\n\u001b[1;32m    627\u001b[0m nveclient\u001b[38;5;241m.\u001b[39mreset_method_cache()\n\u001b[1;32m    628\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    629\u001b[0m     [\n\u001b[1;32m    630\u001b[0m         Model(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayground_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), path\u001b[38;5;241m=\u001b[39mv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mMODEL_SPECS\u001b[38;5;241m.\u001b[39mget(k, {}))\n\u001b[0;32m--> 631\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnveclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavailable_models\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    632\u001b[0m     ],\n\u001b[1;32m    633\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    634\u001b[0m )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:157\u001b[0m, in \u001b[0;36mNVEModel.available_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_available_models \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_available_models\n\u001b[0;32m--> 157\u001b[0m live_fns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavailable_functions\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m live_fns[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    159\u001b[0m     live_fns \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m live_fns \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACTIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:176\u001b[0m, in \u001b[0;36mNVEModel.available_functions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m     query_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(invoke_url)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to query model endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvoke_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(query_res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to query model endpoint https://api.nvcf.nvidia.com/v2/nvcf/functions.\n[401] Unauthorized\nBearer error=\"invalid_token\"\nerror_description=\"Bearer token is malformed\"\nerror_uri=\"https://tools.ietf.org/html/rfc6750#section-3.1\"\nPlease check or regenerate your API key."
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# get all available models\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize LLM model and embeddings\n",
    "- I will be using LLAMA3-8b open-source model and Nvidia embedding model\n",
    "- will also work with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "# embedder = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work with Open-Source models\n",
    "\n",
    "llm = ChatNVIDIA(model='ai-mixtral-8x7b-instruct') #mistralai/mixtral-8x22b-instruct-v0.1, ai-mistral-large, ai-mixtral-8x7b-instruct, ai-llama3-8b\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: load documents using document loaders provided by LangChain\n",
    "\n",
    "- Here, we will be using `UnstructuredPDFLoader` for loading documents in directory \n",
    "\n",
    "- and `OnlinePDFLoader` for loading online documents from the arxiv repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# load document from directory\n",
    "loader = UnstructuredPDFLoader(\"../Generative-AI-Book.pdf\")\n",
    "book = loader.load()\n",
    "\n",
    "# load document from arxiv\n",
    "loader_01 = OnlinePDFLoader(\"https://arxiv.org/pdf/2312.10997\")\n",
    "paper = loader_01.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Storing Embeddings in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "# text splits for both documents\n",
    "chunks_book = text_splitter.split_documents(book)\n",
    "chunks_paper = text_splitter.split_documents(paper)\n",
    "\n",
    "# storing in vector store\n",
    "faiss_db_book = FAISS.from_documents(chunks_book, embedder)\n",
    "faiss_db_paper = FAISS.from_documents(chunks_paper, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Creating Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_book = faiss_db_book.as_retriever()\n",
    "retriever_paper = faiss_db_paper.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retriever_paper.invoke(\"what is Retrieval-Augmented Generation?\")[0].page_content.split('|')\n",
    "# response = '\\n'.join(docs) #?\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Construct Retriever for the Book and the arxiv paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "# Class for Book Retriever\n",
    "class BookRetriever(BaseTool):\n",
    "    name = 'Book'\n",
    "    description = 'Useful for understanding the content of the Generative AI book'\n",
    "    \n",
    "    def _run(self, query):\n",
    "        out = retriever_book.invoke(query)[0].page_content.split('|')\n",
    "        output = '\\n'.join(out)\n",
    "        return output\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "# Class for Paper Retriever\n",
    "class PaperRetriever(BaseTool):\n",
    "    name = 'Paper'\n",
    "    description = 'Useful for understanding the latest on RAG techniques'\n",
    "    \n",
    "    def _run(self, query):\n",
    "        out = retriever_paper.invoke(query)[0].page_content.split('|')\n",
    "        output = '\\n'.join(out)\n",
    "        return output\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "    \n",
    "\n",
    "# initialize the retrievers\n",
    "book_retriever = BookRetriever()\n",
    "paper_retriever = PaperRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load and initialize available tools in Langchain\n",
    "\n",
    "- Arxiv\n",
    "\n",
    "- Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# arxiv tool\n",
    "arxiv =  ArxivAPIWrapper()\n",
    "\n",
    "# wikipedia tool\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "## A proper name and a good description helps to know on how to use the tools\n",
    "wiki_tool = Tool.from_function(\n",
    "    func=wikipedia.run,\n",
    "    name=\"Wiki\",\n",
    "    description=\"useful for when you need to search certain topic on Wikipedia, aka wiki\")\n",
    "\n",
    "arxiv_tool = Tool.from_function(\n",
    "    func=arxiv.run,\n",
    "    name=\"Arxiv\",\n",
    "    description=\"useful for querying from arxiv repository\")\n",
    "\n",
    "book_tool = Tool.from_function(\n",
    "    func= book_retriever.invoke,\n",
    "    name = 'book',\n",
    "    description = 'Useful for understanding the latest Generative AI book'\n",
    ")\n",
    "\n",
    "paper_tool = Tool.from_function(\n",
    "    func= paper_retriever.invoke,\n",
    "    name = 'paper',\n",
    "    description = 'Useful for understanding the RAG from a specific paper'\n",
    ")\n",
    "\n",
    "# List of all tools\n",
    "tools = [book_tool, paper_tool] # wiki_tool, arxiv_tool, book_tool, paper_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Wrap the tools in ToolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: anyio in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpx) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from anyio->httpx) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /opt/miniconda3/envs/.venv/lib/python3.10/site-packages (from anyio->httpx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install httpx\n",
    "from langchain_core.agents import AgentFinish\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor # reinstall langgraph if fails!\n",
    "\n",
    "# This a helper class we have that is useful for running tools\n",
    "# It takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Prompt Template for Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt ---> \n",
      " input_variables=['agent_scratchpad', 'chat_history', 'input'] partial_variables={'tools': ['Search'], 'tool_names': 'Search'} template=\"\\n### [INST]\\n\\nAssistant is a large language model to answer questions on Generative AI.\\n\\n\\nContext:\\n------\\n\\nAssistant has access to the following tools:\\n\\n{tools}\\n\\nTo use a tool, please use the following format:\\n\\n'''\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n'''\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n'''\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n'''\\n\\nBegin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n\\nCurrent Scratchpad:\\n{agent_scratchpad}\\n\\n[/INST]\\n \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentType, Agent, ConversationalAgent\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "## set up memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "\n",
    "Assistant is a large language model to answer questions on Generative AI.\n",
    "\n",
    "\n",
    "Context:\n",
    "------\n",
    "\n",
    "Assistant has access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "'''\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New input: {input}\n",
    "\n",
    "Current Scratchpad:\n",
    "{agent_scratchpad}\n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'chat_history', 'input', 'tool_names', 'tools'],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(\n",
    "    tools=[t.name for t in tools],\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "print(\"prompt ---> \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['SERPAPI_API_KEY']=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPERIMENT ## (to-be deleted) **does not work with open-source LLM model**\n",
    "# from langchain import hub\n",
    "# from langchain.agents import AgentExecutor, load_tools\n",
    "# from langchain.agents.format_scratchpad import format_log_to_str\n",
    "# from langchain.tools.render import render_text_description\n",
    "# from langchain.agents.output_parsers import (\n",
    "#     ReActJsonSingleInputOutputParser,\n",
    "# )\n",
    "\n",
    "# # setup tools\n",
    "# tools = load_tools([\"serpapi\"], llm=llm)\n",
    "\n",
    "# # prompt\n",
    "# prompt = hub.pull(\"hwchase17/react-json\")\n",
    "# prompt = prompt.partial(\n",
    "#     tools=render_text_description(tools),\n",
    "#     tool_names=\", \".join([t.name for t in tools]),\n",
    "# )\n",
    "\n",
    "# # define the agent\n",
    "# chat_model = llm\n",
    "# chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "# agent = (\n",
    "#     {\n",
    "#         \"input\": lambda x: x[\"input\"],\n",
    "#         \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | chat_model_with_stop\n",
    "#     | ReActJsonSingleInputOutputParser()\n",
    "# )\n",
    "\n",
    "# instantiate AgentExecutor\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# # creating agent that has access to the tool list\n",
    "# agent = create_tool_calling_agent(model, tools, prompt) #model\n",
    "\n",
    "# agent_execute = AgentExecutor(agent=agent,\n",
    "#                               tools=tools,\n",
    "#                               verbose=True,\n",
    "#                               max_iterations=3,\n",
    "#                               early_stopping_method=\"generate\"\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"what is retrieval augmented generation?\",\n",
    "        \n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Sequence\n",
    "\n",
    "from langchain_core._api import deprecated\n",
    "from langchain_core.callbacks import BaseCallbackManager\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.loading import AGENT_TO_CLASS, load_agent\n",
    "\n",
    "agent_cls = AGENT_TO_CLASS[AgentType.CONVERSATIONAL_REACT_DESCRIPTION] # didn't understand this!!! CONVERSATIONAL_REACT_DESCRIPTION\n",
    "agent_kwargs = {}\n",
    "agent_obj = agent_cls.from_llm_and_tools(\n",
    "    model, tools, callback_manager=None, **agent_kwargs)\n",
    "\n",
    "agent_execute=AgentExecutor.from_agent_and_tools(\n",
    "        agent=agent_obj,\n",
    "        tools=tools,\n",
    "        callback_manager=None,\n",
    "        # handle_parsing_errors=True,\n",
    "        handle_parsing_errors=\"Check your output and make sure it conforms, use the Action/Action Input syntax\",\n",
    "        verbose=True,\n",
    "        output_key = \"output\",\n",
    "        max_iterations=3,\n",
    "        return_intermediate_steps=True,\n",
    "        early_stopping_method=\"generate\", # or use **force**\n",
    "        memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\"),\n",
    "        # reduce_k_below_max_tokens=True,\n",
    "        # max_tokens = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The input string\n",
    "    input: str\n",
    "    # The list of previous messages in the conversation\n",
    "    chat_history: list[BaseMessage]\n",
    "    # The outcome of a given call to the agent\n",
    "    # Needs `None` as a valid type, since this is what this will start as\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    # List of actions and corresponding observations\n",
    "    # Here we annotate this with `operator.add` to indicate that operations to\n",
    "    # this state should be ADDED to the existing values (not overwrite it)\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "from langchain_core.agents import AgentActionMessageLog\n",
    "\n",
    "def run_agent(data):\n",
    "    inputs = data.copy()\n",
    "    text = inputs['input']\n",
    "    # chat_history = inputs['chat_history']\n",
    "    agent_outcome = agent_execute.invoke({\"input\":text}) #, \"chat_history\": chat_history\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def execute_tools(data):\n",
    "    # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
    "    agent_output = data[\"agent_outcome\"]\n",
    "    if len(agent_output['intermediate_steps'])>=1 :\n",
    "        agent_action = agent_output['intermediate_steps'][0][0]\n",
    "        output = tool_executor.invoke(agent_action)\n",
    "        return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
    "    else:\n",
    "        return {\"intermediate_steps\":[]}\n",
    "\n",
    "# Define logic that is used to determine which conditional edge to go down\n",
    "def should_continue(data):\n",
    "    # If the agent outcome is an AgentFinish, then we return `exit` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    if data[\"agent_outcome\"][\"output\"] is not None:\n",
    "        print(\" **AgentFinish** \" )\n",
    "        return \"end\"\n",
    "    # Otherwise, an AgentAction is returned\n",
    "    # Here we return `continue` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    else:\n",
    "        print(\" **continue** \" )\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` Thought: Do I need to use a tool? Yes\n",
      "\n",
      "Action: paper`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` Do I need to use a tool? Yes\n",
      "\n",
      "Action: paper\n",
      "\n",
      "`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` It seems like there was an issue with the previous output. To explain Adapt`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: ` Thought: Do I need to use a tool? Yes\n\nAction: paper`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain Adaptive Retrieval methods\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;66;03m# ,\"chat_history\": []\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1333\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1335\u001b[0m     config,\n\u001b[1;32m   1336\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1337\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1338\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1339\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1340\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1341\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1345\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:876\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    869\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    870\u001b[0m     futures,\n\u001b[1;32m    871\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    872\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    873\u001b[0m )\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    879\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1422\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1420\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2493\u001b[0m failed_inputs_map: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stepidx, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;66;03m# Assemble the original indexes of the remaining inputs\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m     \u001b[38;5;66;03m# (i.e. the ones that haven't failed yet)\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m     remaining_idxs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2498\u001b[0m         i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(configs)) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m failed_inputs_map\n\u001b[0;32m-> 2499\u001b[0m     ]\n\u001b[1;32m   2500\u001b[0m     \u001b[38;5;66;03m# Invoke the step on the remaining inputs\u001b[39;00m\n\u001b[1;32m   2501\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m   2502\u001b[0m         [\n\u001b[1;32m   2503\u001b[0m             inp\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2517\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/utils.py:89\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     83\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     84\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     85\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[142], line 8\u001b[0m, in \u001b[0;36mrun_agent\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# chat_history = inputs['chat_history']\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m agent_outcome \u001b[38;5;241m=\u001b[39m \u001b[43magent_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, \"chat_history\": chat_history\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent_outcome}\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m--> 163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    148\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    149\u001b[0m     inputs,\n\u001b[1;32m    150\u001b[0m     run_id,\n\u001b[1;32m    151\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    152\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:1455\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1451\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1452\u001b[0m                 tool_return, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1453\u001b[0m             )\n\u001b[1;32m   1454\u001b[0m     iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1455\u001b[0m     time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m   1456\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mreturn_stopped_response(\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_method, intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\n\u001b[1;32m   1458\u001b[0m )\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:868\u001b[0m, in \u001b[0;36mreturn_stopped_response\u001b[0;34m(self, early_stopping_method, intermediate_steps, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m full_inputs = {**kwargs, **new_inputs}\n\u001b[1;32m    867\u001b[0m full_output = self.llm_chain.predict(**full_inputs)\n\u001b[0;32m--> 868\u001b[0m # We try to extract a final answer\n\u001b[1;32m    869\u001b[0m parsed_output = self.output_parser.parse(full_output)\n\u001b[1;32m    870\u001b[0m if isinstance(parsed_output, AgentFinish):\n\u001b[1;32m    871\u001b[0m     # If we can extract, we send the correct stuff\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langchain/agents/conversational/output_parser.py:32\u001b[0m, in \u001b[0;36mConvoOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(regex, text, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m action \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m action_input \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: ` Thought: Do I need to use a tool? Yes\n\nAction: paper`"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"Explain Adaptive Retrieval methods\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `Self-Retrieval-Augmented Generation (Self-RAG) is an advanced technique in generative AI designed to enhance a model's ability to retrieve relevant information autonomously during the text generation process. Here are the main components and functionalities of Self-RAG:\n",
      "\n",
      "1. **Reflection Tokens**: These are special tokens such as \"retrieve\" and \"critic\" that the model uses to introspect and decide when to trigger the retrieval mechanism. The decision can also be influenced by a predefined threshold based on the probability of certain generated terms.\n",
      "\n",
      "2. **Autonomous Retrieval Activation**: The model can independently determine when to activate the retrieval mechanism by analyzing the context and using reflection tokens. This allows the model to recognize the need for additional information to maintain coherence and relevance in the generated text.\n",
      "\n",
      "3. **Fragment-level Beam Search**: During the retrieval phase, Self-RAG performs a fragment-level beam search across multiple paragraphs. This method helps identify the most coherent and relevant sequence of information from the retrieved content.\n",
      "\n",
      "4. **Critic Scores**: These scores are used to dynamically update subdivision scores, allowing the model to adjust weights during the inference process. This adjustment improves the model's decision-making regarding when and how to retrieve information.\n",
      "\n",
      "5. **Streamlined Decision-Making**: Self-RAG simplifies the decision-making process by removing the need for additional classifiers or dependence on Natural Language Inference (NLI). This simplification makes the model more efficient and effective in its retrieval-related decisions.\n",
      "\n",
      "Overall, Self-RAG enhances the retrieval cycle, making the generated text more coherent and relevant by allowing the model to effectively manage its information retrieval processes autonomously.`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Self-Retrieval-Augmented Generation (Self-RAG) is an advanced technique in generative AI designed to enhance a model's ability to autonomously retrieve relevant information during the text generation process. Here are the main components and functionalities of Self-RAG:\n",
      "\n",
      "1. **Reflection Tokens**: These are special tokens such as \"retrieve\" and \"critic\" that the model uses to introspect and decide when to trigger the retrieval mechanism. The decision can also be influenced by a predefined threshold based on the probability of certain generated terms.\n",
      "\n",
      "2. **Autonomous Retrieval Activation**: The model can independently determine when to activate the retrieval mechanism by analyzing the context and using reflection tokens. This allows the model to recognize the need for additional information to maintain coherence and relevance in the generated text.\n",
      "\n",
      "3. **Fragment-level Beam Search**: During the retrieval phase, Self-RAG performs a fragment-level beam search across multiple paragraphs. This method helps identify the most coherent and relevant sequence of information from the retrieved content.\n",
      "\n",
      "4. **Critic Scores**: These scores are used to dynamically update subdivision scores, allowing the model to adjust weights during the inference process. This adjustment improves the model's decision-making regarding when and how to retrieve information.\n",
      "\n",
      "5. **Streamlined Decision-Making**: Self-RAG simplifies the decision-making process by removing the need for additional classifiers or dependence on Natural Language Inference (NLI). This simplification makes the model more efficient and effective in its retrieval-related decisions.\n",
      "\n",
      "Overall, Self-RAG enhances the retrieval cycle, making the generated text more coherent and relevant by allowing the model to effectively manage its information retrieval processes autonomously.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you explain Self-RAG?\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;66;03m# ,\"chat_history\": []\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1333\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1335\u001b[0m     config,\n\u001b[1;32m   1336\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1337\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1338\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1339\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1340\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1341\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1345\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:869\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    862\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    863\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(run_with_retry, task, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy)\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m next_tasks\n\u001b[1;32m    865\u001b[0m ]\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    876\u001b[0m _panic_or_proceed(done, inflight, step)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/miniconda3/envs/.venv/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"Can you explain Self-RAG?\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: One advantage of Self-RAG is that it allows the model to autonomously determine when and how to retrieve relevant information, leading to more coherent and contextually accurate text generation. This reduces the need for manual intervention and makes the model more efficient.\n",
      "\n",
      "One disadvantage of Self-RAG is the potential computational complexity and resource requirements involved in performing fragment-level beam searches and dynamic updates with critic scores. This can make the model slower and more resource-intensive, potentially limiting its scalability for large-scale applications.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"what is one advantage and one disadvantage of Self-RAG?\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `Scaling laws in the context of Retrieval-Augmented Generation (RAG) refer to how the performance of RAG models changes as a function of various factors such as model size, dataset size, and computational resources. These laws help researchers and practitioners understand the trade-offs and benefits of scaling up different components of the RAG system. Here are some key aspects of scaling laws in RAG:\n",
      "\n",
      "1. **Model Size**: Increasing the size of the generative model typically leads to better performance in terms of generating coherent and relevant text. Larger models have more capacity to understand and generate complex patterns in the data. However, this comes at the cost of increased computational resources and memory requirements.\n",
      "\n",
      "2. **Dataset Size**: The amount of data available for both training and retrieval is crucial. A larger dataset can provide more diverse and relevant information for the model to retrieve, which can significantly improve the model's performance. However, managing and processing larger datasets also require more storage and computational power.\n",
      "\n",
      "3. **Retrieval Mechanism**: The efficiency and accuracy of the retrieval mechanism itself are vital. As the retrieval component becomes more sophisticated (e.g., through fragment-level beam search), the quality of the information retrieved improves, but this also increases computational complexity and latency.\n",
      "\n",
      "4. **Compute Resources**: The amount of computational power available affects how quickly and efficiently the model can be trained and deployed. More computational resources can enable the use of larger models and larger datasets, but they also come with higher costs.\n",
      "\n",
      "5. **Performance Trade-offs**: There is often a trade-off between the different factors. For instance, while a larger model might improve performance, it may also require disproportionately more computational resources, leading to diminishing returns. Similarly, while increasing the dataset size can improve retrieval quality, it may also slow down the retrieval process.\n",
      "\n",
      "Understanding these scaling laws helps in making informed decisions about how to allocate resources effectively to achieve the desired balance between performance and efficiency in RAG systems.`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Scaling laws in the context of Retrieval-Augmented Generation (RAG) refer to how the performance of RAG models changes as a function of various factors such as model size, dataset size, and computational resources. These laws help researchers and practitioners understand the trade-offs and benefits of scaling up different components of the RAG system. Here are some key aspects of scaling laws in RAG:\n",
      "\n",
      "1. **Model Size**: Increasing the size of the generative model typically leads to better performance in terms of generating coherent and relevant text. Larger models have more capacity to understand and generate complex patterns in the data. However, this comes at the cost of increased computational resources and memory requirements.\n",
      "\n",
      "2. **Dataset Size**: The amount of data available for both training and retrieval is crucial. A larger dataset can provide more diverse and relevant information for the model to retrieve, which can significantly improve the model's performance. However, managing and processing larger datasets also require more storage and computational power.\n",
      "\n",
      "3. **Retrieval Mechanism**: The efficiency and accuracy of the retrieval mechanism itself are vital. As the retrieval component becomes more sophisticated (e.g., through fragment-level beam search), the quality of the information retrieved improves, but this also increases computational complexity and latency.\n",
      "\n",
      "4. **Compute Resources**: The amount of computational power available affects how quickly and efficiently the model can be trained and deployed. More computational resources can enable the use of larger models and larger datasets, but they also come with higher costs.\n",
      "\n",
      "5. **Performance Trade-offs**: There is often a trade-off between the different factors. For instance, while a larger model might improve performance, it may also require disproportionately more computational resources, leading to diminishing returns. Similarly, while increasing the dataset size can improve retrieval quality, it may also slow down the retrieval process.\n",
      "\n",
      "Understanding these scaling laws helps in making informed decisions about how to allocate resources effectively to achieve the desired balance between performance and efficiency in RAG systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"explain the Scaling laws of RAG?\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `Flowise AI focuses on providing tools and frameworks to make Retrieval-Augmented Generation (RAG) production-ready, enabling seamless integration into real-world applications. Here are some key areas where Flowise AI contributes:\n",
      "\n",
      "1. **Optimized Infrastructure**: Flowise AI offers optimized infrastructure to handle the computational demands of RAG models. This includes scalable cloud solutions and efficient data pipelines that can manage large datasets and high-throughput retrieval operations.\n",
      "\n",
      "2. **Pre-trained Models**: Flowise AI provides access to pre-trained RAG models that are fine-tuned on diverse datasets. These models are ready to be deployed and can be further customized to specific use cases, reducing the time and effort required for model training.\n",
      "\n",
      "3. **Efficient Retrieval Mechanisms**: Flowise AI implements advanced retrieval mechanisms such as vector search and fragment-level beam search, which are optimized for speed and accuracy. These mechanisms are designed to retrieve the most relevant information quickly, even from large datasets.\n",
      "\n",
      "4. **Scalability**: The platform is built to scale horizontally, allowing businesses to handle growing data and user loads without compromising performance. This includes distributed computing frameworks and load balancing techniques to ensure smooth operation under high demand.\n",
      "\n",
      "5. **Integration and Deployment**: Flowise AI provides APIs and SDKs that make it easy to integrate RAG models into existing systems. This includes support for various programming languages and frameworks, enabling seamless deployment in production environments.\n",
      "\n",
      "6. **Monitoring and Analytics**: To ensure reliability and performance, Flowise AI offers tools for monitoring and analyzing the behavior of RAG models in production. This includes tracking metrics such as response time, retrieval accuracy, and resource utilization.\n",
      "\n",
      "7. **Security and Compliance**: Flowise AI ensures that data handling and processing comply with industry standards and regulations. This includes data encryption, access controls, and regular security audits to protect sensitive information.\n",
      "\n",
      "By addressing these critical aspects, Flowise AI makes it easier for organizations to deploy and manage RAG systems in production, ensuring they are robust, scalable, and efficient.`\u001b[0m\n",
      "Observation: Check your output and make sure it conforms, use the Action/Action Input syntax\n",
      "Thought:\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Flowise AI focuses on providing tools and frameworks to make Retrieval-Augmented Generation (RAG) production-ready, enabling seamless integration into real-world applications. Here are some key areas where Flowise AI contributes:\n",
      "\n",
      "1. **Optimized Infrastructure**: Flowise AI offers optimized infrastructure to handle the computational demands of RAG models. This includes scalable cloud solutions and efficient data pipelines that can manage large datasets and high-throughput retrieval operations.\n",
      "\n",
      "2. **Pre-trained Models**: Flowise AI provides access to pre-trained RAG models that are fine-tuned on diverse datasets. These models are ready to be deployed and can be further customized to specific use cases, reducing the time and effort required for model training.\n",
      "\n",
      "3. **Efficient Retrieval Mechanisms**: Flowise AI implements advanced retrieval mechanisms such as vector search and fragment-level beam search, which are optimized for speed and accuracy. These mechanisms are designed to retrieve the most relevant information quickly, even from large datasets.\n",
      "\n",
      "4. **Scalability**: The platform is built to scale horizontally, allowing businesses to handle growing data and user loads without compromising performance. This includes distributed computing frameworks and load balancing techniques to ensure smooth operation under high demand.\n",
      "\n",
      "5. **Integration and Deployment**: Flowise AI provides APIs and SDKs that make it easy to integrate RAG models into existing systems. This includes support for various programming languages and frameworks, enabling seamless deployment in production environments.\n",
      "\n",
      "6. **Monitoring and Analytics**: To ensure reliability and performance, Flowise AI offers tools for monitoring and analyzing the behavior of RAG models in production. This includes tracking metrics such as response time, retrieval accuracy, and resource utilization.\n",
      "\n",
      "7. **Security and Compliance**: Flowise AI ensures that data handling and processing comply with industry standards and regulations. This includes data encryption, access controls, and regular security audits to protect sensitive information.\n",
      "\n",
      "By addressing these critical aspects, Flowise AI makes it easier for organizations to deploy and manage RAG systems in production, ensuring they are robust, scalable, and efficient.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " **AgentFinish** \n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"explain what is Flowise AI doing for production-ready RAG?\"} # ,\"chat_history\": []\n",
    "outputs = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flowise AI focuses on providing tools and frameworks to make Retrieval-Augmented Generation (RAG) production-ready, enabling seamless integration into real-world applications. Here are some key areas where Flowise AI contributes:\\n\\n1. **Optimized Infrastructure**: Flowise AI offers optimized infrastructure to handle the computational demands of RAG models. This includes scalable cloud solutions and efficient data pipelines that can manage large datasets and high-throughput retrieval operations.\\n\\n2. **Pre-trained Models**: Flowise AI provides access to pre-trained RAG models that are fine-tuned on diverse datasets. These models are ready to be deployed and can be further customized to specific use cases, reducing the time and effort required for model training.\\n\\n3. **Efficient Retrieval Mechanisms**: Flowise AI implements advanced retrieval mechanisms such as vector search and fragment-level beam search, which are optimized for speed and accuracy. These mechanisms are designed to retrieve the most relevant information quickly, even from large datasets.\\n\\n4. **Scalability**: The platform is built to scale horizontally, allowing businesses to handle growing data and user loads without compromising performance. This includes distributed computing frameworks and load balancing techniques to ensure smooth operation under high demand.\\n\\n5. **Integration and Deployment**: Flowise AI provides APIs and SDKs that make it easy to integrate RAG models into existing systems. This includes support for various programming languages and frameworks, enabling seamless deployment in production environments.\\n\\n6. **Monitoring and Analytics**: To ensure reliability and performance, Flowise AI offers tools for monitoring and analyzing the behavior of RAG models in production. This includes tracking metrics such as response time, retrieval accuracy, and resource utilization.\\n\\n7. **Security and Compliance**: Flowise AI ensures that data handling and processing comply with industry standards and regulations. This includes data encryption, access controls, and regular security audits to protect sensitive information.\\n\\nBy addressing these critical aspects, Flowise AI makes it easier for organizations to deploy and manage RAG systems in production, ensuring they are robust, scalable, and efficient.\\n```'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['agent_outcome']['output']\n",
    "# outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
